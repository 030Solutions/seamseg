# Seamless Scene Segmentation

<p align="center">
<img src="network.png" width="70%"/>
<br>
<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Porzi_Seamless_Scene_Segmentation_CVPR_2019_paper.html">CVPR</a>
|
<a href="https://arxiv.org/abs/1905.01220">arXiv</a>
</p>

Seamless Scene Segmentation is a CNN-based architecture that can be trained end-to-end to predict a complete class- and
instance-aware labeling of each pixel in an image. To tackle this task, also known as "panoptic segmentation", we take
advantage of a novel segmentation head that seamlessly integrates multi-scale features generated by a Feature Pyramid
Network with contextual information conveyed by a light-weight DeepLab-like module.

This repository contains training and evaluation Pytorch code, based on our re-implementation of the Mask R-CNN
framework. Pre-trained models to reproduce the Mapillary Vistas results from our paper will be published soon.

If you use Seamless Scene Segmentation in your research, please cite:
```bibtex
@inProceedings{porzi2019seamless,
  author    = {Porzi, Lorenzo and Rota Bul\`o, Samuel and Colovic, Aleksander and Kontschieder, Peter},
  title     = {Seamless Scene Segmentation},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019},
  month     = {June}
}
```

## Requirements and setup

Main system requirements:
* CUDA 10.1
* Linux with GCC 7 or 8
* PyTorch v1.1.0

**IMPORTANT NOTE**: these requirements are not necessarily stringent, e.g. it might be possible to compile with older
versions of CUDA, or under Windows. However, we are not able to test or provide any support for other setups.

To install PyTorch, please refer to https://github.com/pytorch/pytorch#installation.

To install all other dependencies using pip:
```bash
pip install -r requirements.txt
```

### Setup

Our code is split into two main components: a library containing implementations for the various network modules,
algorithms and utilities, and a set of scripts to train / test the networks.

The library, called `seamseg`, can be installed with:
```bash
git clone https://github.com/mapillary/seamseg.git
cd seamseg
python setup.py install
```
or, in a single line:
```bash
pip install git+https://github.com/mapillary/seamseg.git
```

The scripts do not require installation (but they *do* require `seamseg` to be installed), and can be run normally
from the `scripts/` folder. *Note:* do not run the scripts from the main folder of this repo, otherwise python might
decide to load the local copy of the `seamseg` package instead of the one installed above, causing issues.

## Using the scripts

Our code uses an intermediate data format to ease training on multiple datasets, described
[here](wiki/Seamless-Scene-Segmentation-dataset-format).
We provide pre-made scripts to convert from [Cityscapes](scripts/data_preparation/prepare_cityscapes.py) and
[Mapillary Vistas](scripts/data_preparation/prepare_vistas.py) to our format.

When training, unless explicitly training from scratch, it's also necessary to convert the ImageNet pre-trained weights
provided by PyTorch to our network format.
To do this, simply run:
```bash
cd scripts/utility
python convert_pytorch_resnet.py NET_NAME OUTPUT_FILE
```
where `NET_NAME` is one of `resnet18`, `resnet34`, `resnet50`, `resnet101` or `resnet152`.

### Training

Training involves three main steps: preparing the dataset, creating a configuration file and running the training
script.
To prepare the dataset, refer to the format description [here](wiki/Seamless-Scene-Segmentation-dataset-format), or
use one of the scripts in [scripts/data_preparation](scripts/data_preparation).
The configuration file is a simple text file in `ini` format.
The default value of each configuration parameter, as well as a short description of what it does, is available in
[seamseg/config/defaults](seamseg/config/defaults).

To launch the training:
```bash
cd scripts
python -m torch.distributed.launch --nproc_per_node=N_GPUS train_panoptic.py --log_dir LOG_DIR CONFIG DATA_DIR 
```
Note that, for now, our code **must** be launched in "distributed" mode using PyTorch's `torch.distributed.launch`
utility.
It's also highly recommended to train on multiple GPUs (possibly 4-8) in order to obtain good results.
Training logs, both in text and Tensorboard formats, will be written in `LOG_DIR`.

### Running inference

Given a trained network, inference can be run on any set of images using
[scripts/test_panoptic.py](scripts/test_panoptic.py):
```bash
cd scripts
python -m torch.distributed.launch --nproc_per_node=N_GPUS test_panoptic.py --meta METADATA --log_dir LOG_DIR CONFIG MODEL INPUT_DIR OUTPUT_DIR
```
Images (either `png` or `jpg`) will be read from `INPUT_DIR` and recursively in its subfolders, and predictions will be
written to `OUTPUT_DIR`.
The script also requires to be given the `metadata.bin` file of the dataset the network was originally trained on.
Note that the script will only read from the `"meta"` section, meaning that a stripped-down version of `metadata.bin`,
i.e. without the `"images"` section, can also be used.